package com.backpackr

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

object Main {
  def main(args: Array[String]): Unit = {
    implicit val spark = SparkSession.builder()
      .appName("backpackr_proj")
      .master("local[*]")
      .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")

    /**
     * Schema ì¶œë ¥
     */
//    // 2019-Oct.csv íŒŒì¼ ì½ê³  ìŠ¤í‚¤ë§ˆ ì¶œë ¥
//    val octDF = spark.read
//      .option("header", "true")
//      .option("inferSchema", "true")
//      .csv("2019-Oct.csv")
//
//    println("[2019-Oct.csv ìŠ¤í‚¤ë§ˆ] : ")
//    octDF.printSchema()
//
//    // 2019-Nov.csv íŒŒì¼ ì½ê³  ìŠ¤í‚¤ë§ˆ ì¶œë ¥
//    val novDF = spark.read
//      .option("header", "true")
//      .option("inferSchema", "true")
//      .csv("2019-Nov.csv")
//
//    println("[2019-Nov.csv ìŠ¤í‚¤ë§ˆ] : ")
//    novDF.printSchema()
    // Step 1. KST ê¸°ì¤€ daily partition ì²˜ë¦¬
    // 1. ë°ì´í„° ë¡œë“œ (Oct + Nov)

    // ğŸ”¹ Spark SQL í‘œí˜„ì‹ ì‚¬ìš©ì„ ìœ„í•œ ì„í¬íŠ¸
    import spark.implicits._

    // 1. Oct + Nov ë‘ íŒŒì¼ì„ í•œë²ˆì— ì½ê¸°
    val df = spark.read
      .option("header", "true")
      .option("inferSchema", "true")
      .csv(Seq("2019-Oct.csv", "2019-Nov.csv"): _*) // ë¦¬ìŠ¤íŠ¸ë¡œ ì—¬ëŸ¬ íŒŒì¼ ì½ê¸°

    // Step 1: KST ê¸°ì¤€ daily partition ì²˜ë¦¬
    val partitionedDF = PartitionWriter.addKSTPartitionColumns(df)(spark)
//    partitionedDF.select("*").show(5, truncate = false)
//    // 3. ê²°ê³¼ í™•ì¸ (íŠ¹ì • ë‚ ì§œë§Œ í™•ì¸)
//    partitionedDF
//      .filter($"partition_date" === "20191002")
//      .select("event_time", "event_time_kst", "partition_date")
//      .show(20, truncate = false)

    // 4. ì „ì²´ row ìˆ˜ í™•ì¸
    val totalCount = partitionedDF.count()
    println(s"âœ… ì „ì²´ row ìˆ˜: $totalCount")

    // Step 2: ë™ì¼ user_idë‚´ì—ì„œ event_time ê°„ê²©ì´ 5ë¶„ ì´ìƒì¸ ê²½ìš° ì„¸ì…˜ ì¢…ë£Œë¡œ ê°„ì£¼í•˜ê³  ìƒˆë¡œìš´ ì„¸ì…˜ IDë¥¼ ìƒì„±
    val sessionizedDF = SessionGenerator.addSessionColumn(partitionedDF)

    // í™•ì¸ìš© ì¶œë ¥
    sessionizedDF.select("user_id", "event_time", "event_time_kst", "session_id")
      .show(20, truncate = false)

    // Step 3 : ì¬ì²˜ë¦¬ í›„ parquet, snappy ì²˜ë¦¬
//    StorageWriter.writeAsPartitionedParquet(sessionizedDF, "output/backpackr_parquet-snappy")

    spark.stop()
  }
}
